{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#########################\n",
    "# Imports \n",
    "#########################\n",
    "import sys, os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets, models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pypianoroll\n",
    "from pypianoroll import Multitrack, Track\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "has_gpu = torch.cuda.is_available()\n",
    "has_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "# Constant \n",
    "#########################\n",
    "RESOLUTION = 24\n",
    "PITCH = 128\n",
    "TS = 4\n",
    "BAR = RESOLUTION\n",
    "MEASURE = BAR * TS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "# Bar Collection\n",
    "#########################\n",
    "\"\"\"\n",
    "    Bar Collection:\n",
    "        A collection of multiple training points on the granularity of \n",
    "        a music bar.\n",
    "        \n",
    "        For simplicity we assume that all Midi input files share the\n",
    "        following properties\n",
    "        \n",
    "        Resolution: 24 (per beat)\n",
    "        Tempo: 120 bpm\n",
    "        Time Signature: 4/4\n",
    "        Note Pitch: [0-127] (128 possibilities)\n",
    "        # Tracks: 1 (Single-Track Midi)\n",
    "            - If Midi contains multiple tracks, use only 1st track\n",
    "        \n",
    "\"\"\"\n",
    "def midi_to_array(path):\n",
    "    \"\"\"\n",
    "        midi_to_array: Returns binarized midi represention of input file\n",
    "            as numpy.ndarry\n",
    "        \n",
    "        Args:\n",
    "            path(str): Path to target midi file\n",
    "        \n",
    "        Returns:\n",
    "            data (np.ndarry): Matrix representation of midi file\n",
    "    \"\"\"\n",
    "    # Import midi data to pypianoroll Multitrack\n",
    "    data = pypianoroll.read(path)\n",
    "\n",
    "    # Export Multitrack to numpy ndarray\n",
    "    data = data.stack() #(N x T x P)\n",
    "    \n",
    "    # Select 1st track if other tracks present\n",
    "    data = np.expand_dims(data[0,:,:], axis=0)\n",
    "    \n",
    "    # Set all velocity values to zero to binarize data\n",
    "    data[data >= 1] = 1\n",
    "    \n",
    "    return data\n",
    "\n",
    "def parse_data(datadir):\n",
    "    \"\"\"\n",
    "        parse_data: Reads all midi files from a directory to produce a\n",
    "            bar collection\n",
    "        \n",
    "        Args:\n",
    "            datadir(str): Directory to import data from\n",
    "        \n",
    "        Return:\n",
    "            bar_collection (np.ndarry: N x T x P): Resulting collection\n",
    "                from file directory\n",
    "        \n",
    "    \"\"\"\n",
    "    midi_list = []\n",
    "    \n",
    "    for root, dirs, filenames in os.walk(datadir):\n",
    "        for filename in filenames:\n",
    "            if filename.endswith(\".mid\") or filename.endswith(\".midi\"):\n",
    "                path = os.path.join(root, filename)\n",
    "                midi_data = midi_to_array(path)\n",
    "                midi_list.append(midi_data)\n",
    "    \n",
    "    print(\"Loaded {} files from directory: {}\".format(len(midi_list), datadir))\n",
    "    \n",
    "    # Concatenate arrays allong time (T) dimension \n",
    "    bar_concat = np.concatenate(midi_list, axis=1) # (1 x T x H)\n",
    "    num_bars = bar_concat.shape[1] / RESOLUTION\n",
    "    \n",
    "    print(\"Resulting Collection has a total of {} bars\".format(int(num_bars)))\n",
    "    \n",
    "    # Process collection for training \n",
    "    bar_concat = np.transpose(bar_concat, axes=(0,2,1)) # (1 x H x T)\n",
    "    bars = np.array_split(bar_concat, num_bars, axis=2) # List (N): (H x W)\n",
    "    bars = [np.expand_dims(bar, axis=0) for bar in bars] # List (N): (1 x H x W)\n",
    "    bar_collection = np.concatenate(bars, axis=0) # (N, 1, H, W)\n",
    "    \n",
    "    return bar_collection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#########################\n",
    "# Dataset\n",
    "#########################\n",
    "class BarDataset(Dataset):\n",
    "    def __init__(self, collection, step_size=BAR):\n",
    "        self.data, self.data_len = self.extract_data(collection)\n",
    "        self.step_size = step_size\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.data_len\n",
    "    \n",
    "    @property\n",
    "    def shape(self):\n",
    "        return self.data.shape\n",
    "    \n",
    "    def __getitem__(self, idx): \n",
    "        X = self.data[idx]\n",
    "        X_prev = torch.zeros(X.shape).cuda() if idx == 0 else self.data[idx-1]\n",
    "        \n",
    "        return X, X_prev\n",
    "    \n",
    "    def extract_data(self, collection):\n",
    "        data_len = collection.shape[0]\n",
    "        \n",
    "        # Transform data to appropriate format\n",
    "        data = torch.Tensor(collection)\n",
    "        data = data.cuda()\n",
    "        \n",
    "        return data, data_len\n",
    "        \n",
    "train = BarDataset(test)\n",
    "train[1][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2 files from directory: ../dataset\n",
      "Resulting Collection has a total of 542 bars\n"
     ]
    }
   ],
   "source": [
    "#########################\n",
    "# Dataloader\n",
    "#########################\n",
    "train_data = parse_data(\"../dataset\")\n",
    "train_dataset = BarDataset(train_data)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, \n",
    "                        num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "# Model Defintion \n",
    "###########################\n",
    "\n",
    "##########################\n",
    "# Model Helpers\n",
    "##########################\n",
    "\n",
    "def conv_prev_concat(x, y):\n",
    "        \"\"\"Concatenate conditioning vector on feature map axis.\"\"\"\n",
    "        x_shapes = x.shape\n",
    "        y_shapes = y.shape\n",
    "        if x_shapes[2:] == y_shapes[2:]:\n",
    "            y2 = y.expand(x_shapes[0],y_shapes[1],x_shapes[2],x_shapes[3])\n",
    "\n",
    "            return torch.cat((x, y2),1)\n",
    "\n",
    "        else:\n",
    "            print(x_shapes[2:])\n",
    "            print(y_shapes[2:])\n",
    "\n",
    "\n",
    "##########################\n",
    "# Model Subunits\n",
    "##########################\n",
    "            \n",
    "class LConv2d(nn.Module):\n",
    "    def __init__(self, c_in, c_out, k, s, p):\n",
    "        super(LConv2d, self).__init__()\n",
    "        self.conv = nn.Conv2d(c_in, c_out, k, s, p, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(c_out)\n",
    "        self.lrelu = nn.LeakyReLU(negative_slope=0.2, inplace=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.lrelu(x)\n",
    "        return x\n",
    "    \n",
    "class ConvTranspose2d(nn.Module):\n",
    "    def __init__(self, c_in, c_out, k, s, p):\n",
    "        super(ConvTranspose2d, self).__init__()\n",
    "        self.conv = nn.ConvTranspose2d(c_in, c_out, k, s, p, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(c_out)\n",
    "        self.relu = nn.ReLU(inplace=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "        \n",
    "           \n",
    "##########################\n",
    "# Model \n",
    "##########################        \n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, gf_dim=64, nz=100, pitch_range=PITCH, bar_length=BAR):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        # Define class properties\n",
    "        self.gf_dim = gf_dim \n",
    "        self.nz = nz # length of input vector 'z' (noise signal)\n",
    "        self.pitch_range = pitch_range\n",
    "        self.num_filters = 256\n",
    "        \n",
    "        # Noise Projection Layer\n",
    "        self.h0_prev = LConv2d(c_in=1, c_out=self.num_filters, k=(1,pitch_range), s=(1,2), p=0)\n",
    "        self.h1_prev = LConv2d(c_in=self.num_filters, c_out=self.num_filters, k=(2,1), s=(2,2), p=0)\n",
    "        self.h2_prev = LConv2d(c_in=self.num_filters, c_out=self.num_filters, k=(2,1), s=(2,2), p=0)\n",
    "        self.h3_prev = LConv2d(c_in=self.num_filters, c_out=self.num_filters, k=(2,1), s=(2,2), p=0)\n",
    "        \n",
    "        # Conditions Layer\n",
    "        self.h1 = ConvTranspose2d(c_in=384, c_out=pitch_range, k=(2,1), s=(2,2), p=0)\n",
    "        self.h2 = ConvTranspose2d(c_in=384, c_out=pitch_range, k=(2,1), s=(2,2), p=0)\n",
    "        self.h3 = ConvTranspose2d(c_in=384, c_out=pitch_range, k=(2,1), s=(2,2), p=0)\n",
    "        self.h4 = ConvTranspose2d(c_in=384, c_out=1, k=(1,pitch_range), s=(1,2), p=0)\n",
    "        \n",
    "        # Linear Transformation layer\n",
    "        self.linear1 = nn.Linear(self.nz, 1024)\n",
    "        self.linear2 = nn.Linear(1024, self.gf_dim*3*2*1)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, z, prev_x):\n",
    "        \n",
    "        prev_x = prev_x.permute(0,1,3,2)\n",
    "        b_size = prev_x.shape[0]\n",
    "        \n",
    "        # TODO: comment on shape of output\n",
    "        h0_prev = self.h0_prev(prev_x)\n",
    "        h1_prev = self.h1_prev(h0_prev)\n",
    "        h2_prev = self.h2_prev(h1_prev)\n",
    "        h3_prev = self.h3_prev(h2_prev)\n",
    "    \n",
    "        \n",
    "        # TODO: comment on shape of output\n",
    "        h0 = self.linear1(z)\n",
    "        \n",
    "        h1 = self.linear2(h0)\n",
    "        h1 = h1.view(b_size, self.gf_dim * 2, 3, 1)\n",
    "        h1 = conv_prev_concat(h1, h3_prev)\n",
    "        \n",
    "        h2 = self.h1(h1)\n",
    "        h2 = conv_prev_concat(h2, h2_prev)\n",
    "        \n",
    "        h3 = self.h2(h2)\n",
    "        h3 = conv_prev_concat(h3, h1_prev)\n",
    "        \n",
    "        h4 = self.h3(h3)\n",
    "        h4 = conv_prev_concat(h4, h0_prev)\n",
    "        \n",
    "        g_x = self.sigmoid(self.h4(h4))\n",
    "        print(g_x.shape)\n",
    "        \n",
    "        return g_x\n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, df_dim=64, dfc_dim=1024, pitch_range=PITCH, bar_length=BAR):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        self.df_dim = df_dim\n",
    "        self.dfc_dim = dfc_dim\n",
    "        self.pitch_range = pitch_range\n",
    "        self.linear_in = self.df_dim * 40 * 15 # (conv kernel output (H,W))\n",
    "        \n",
    "        self.h0 = LConv2d(c_in=1, c_out=64, k=(4,89), s=1, p=0)\n",
    "        self.h1 = LConv2d(c_in=64, c_out=64, k=(4,1), s=1, p=0)\n",
    "        self.h2 = LConv2d(c_in=64, c_out=64, k=(4,1), s=1, p=0)\n",
    "        \n",
    "        self.linear = nn.Linear(self.linear_in, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        #self.linear1 = nn.Linear(200, self.dfc_dim)\n",
    "        #self.bn1 = nn.BatchNorm1d(self.dfc_dim)\n",
    "        \n",
    "        #self.linear2 = nn.Linear(self.dfc_dim, 1)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        print(x.shape)\n",
    "        #x = x.permute(0,1,3,2)\n",
    "        b_size = x.shape[0]\n",
    "        \n",
    "        h0 = self.h0(x)\n",
    "        h1 = self.h1(h0)\n",
    "        h2 = self.h2(h1)\n",
    "        h2 = h2.reshape(b_size, self.linear_in)\n",
    "        h3 = self.linear(h2)\n",
    "        \n",
    "        h3_sigmoid = self.sigmoid(h3)\n",
    "        \n",
    "        return h3_sigmoid, h3     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1, 24, 128])\n",
      "torch.Size([16, 1, 24, 128])\n",
      "torch.Size([16, 1, 24, 128])\n",
      "torch.Size([16, 1, 24, 128])\n",
      "torch.Size([16, 1, 24, 128])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [1]] is at version 2; expected version 1 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-295-906452ad5d34>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m             \u001b[1;31m# Calculate gradients for G\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m             \u001b[0merrG1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m             \u001b[0mD_G_z2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\handout1\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m         \"\"\"\n\u001b[1;32m--> 185\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\handout1\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 125\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[1;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [1]] is at version 2; expected version 1 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!"
     ]
    }
   ],
   "source": [
    "##########################\n",
    "# Training Functions \n",
    "##########################\n",
    "lr = 1e-3\n",
    "epochs = 20\n",
    "nz = 100\n",
    "\n",
    "# Model instantiation\n",
    "modelG = Generator(nz=nz)\n",
    "modelD = Discriminator()\n",
    "modelG.to(device)\n",
    "modelD.to(device)\n",
    "\n",
    "# Model optimizers\n",
    "optG = torch.optim.Adam(modelG.parameters(), lr=lr)\n",
    "optD = torch.optim.Adam(modelD.parameters(), lr=lr)\n",
    "\n",
    "# Model Criterion\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Establish convention for real and fake labels during training\n",
    "real_label = 1.\n",
    "fake_label = 0.\n",
    "\n",
    "# Random Noise \n",
    "z = torch.randn(batch_size, 100, device=device)\n",
    "\n",
    "# Loss accumulators\n",
    "average_lossD = 0\n",
    "average_lossG = 0\n",
    "average_D_x   = 0\n",
    "average_D_G_z = 0\n",
    "\n",
    "lossD_list =  []\n",
    "lossD_list_all = []\n",
    "lossG_list =  []\n",
    "lossG_list_all = []\n",
    "D_x_list = []\n",
    "D_G_z_list = []\n",
    "\n",
    "with torch.autograd.set_detect_anomaly(True):\n",
    "    for epoch in range(epochs):\n",
    "        sum_lossG = 0\n",
    "        sum_lossD = 0\n",
    "        sum_D_x = 0\n",
    "        sum_D_g_z = 0\n",
    "\n",
    "        for i, (X, X_prev) in enumerate(train_loader):\n",
    "            ############################\n",
    "            # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "            ###########################\n",
    "\n",
    "            # train with real samples\n",
    "            modelD.zero_grad()\n",
    "            X = X.to(device)\n",
    "            X_prev = X_prev.to(device)\n",
    "\n",
    "            # Format batch\n",
    "            b_size = X.size(0)\n",
    "            label = torch.full((b_size,), real_label, dtype=torch.float, device=device) # Create real labels\n",
    "\n",
    "            # Forward pass real batch through D\n",
    "            X = X.permute(0,1,3,2) # Permutate tensor to produce correct shape\n",
    "            out, out_logits = modelD(X)\n",
    "\n",
    "            # Calculate loss on all-real batch\n",
    "            d_loss_real = criterion(out, label)\n",
    "\n",
    "            # Calculate gradients for D in backward pass\n",
    "            d_loss_real.backward(retain_graph=True)\n",
    "            D_x = out.mean().item()\n",
    "            sum_D_x += D_x \n",
    "\n",
    "            ## Train with all-fake batch\n",
    "            # Generate batch of latent vectors\n",
    "            noise = torch.randn(b_size, nz, device=device)\n",
    "\n",
    "            # Generate fake image batch with G\n",
    "            fake = modelG(noise, X_prev)\n",
    "            label.fill_(fake_label)\n",
    "\n",
    "            # Classify all fake batch with D\n",
    "            out, out_logits = modelD(fake.detach())\n",
    "\n",
    "            # Calculate D's loss on the all-fake batch\n",
    "            d_loss_fake = criterion(out, label)\n",
    "\n",
    "            # Calculate the gradients for this batch\n",
    "            d_loss_fake.backward(retain_graph=True)\n",
    "            D_G_z1 = out.mean().item()\n",
    "\n",
    "            # Add the gradients from the all-real and all-fake batches\n",
    "            errD = d_loss_real + d_loss_fake\n",
    "            errD = errD.item()\n",
    "\n",
    "            # Update D\n",
    "            optD.step()\n",
    "\n",
    "            ############################\n",
    "            # (2) Update G network: maximize log(D(G(z)))\n",
    "            ###########################\n",
    "            modelG.zero_grad()\n",
    "            label.fill_(real_label) # fake labels are real for generator cost\n",
    "\n",
    "            # Since we just updated D, perform another forward pass of all-fake batch through D\n",
    "            out, out_logits = modelD(fake)\n",
    "\n",
    "            # Calculate G's loss based on this output\n",
    "            errG = criterion(out, label)\n",
    "            sum_lossG += errG\n",
    "\n",
    "            # Calculate gradients for G\n",
    "            errG.backward(retain_graph=True)\n",
    "\n",
    "            D_G_z2 = out.mean().item()\n",
    "            # Update G\n",
    "            optG.step()\n",
    "\n",
    "            ############################\n",
    "            # (3) Update G network again: maximize log(D(G(z)))\n",
    "            # Done to mitigate strength of Discriminator model \n",
    "            ###########################\n",
    "            modelG.zero_grad()\n",
    "            label.fill_(real_label) \n",
    "\n",
    "            # Since we just updated D, perform another forward pass of all-fake batch through D\n",
    "            out, out_logits = modelD(fake)\n",
    "\n",
    "            # Calculate G's loss based on this output\n",
    "            errG = criterion(out, label)\n",
    "\n",
    "            # Calculate gradients for G\n",
    "            errG.backward(retain_graph=True)\n",
    "\n",
    "            D_G_z2 = out.mean().item()\n",
    "            sum_D_G_z += D_G_z2\n",
    "            # Update G\n",
    "            optG.step()\n",
    "\n",
    "            if epoch % 5 == 0:\n",
    "                print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f / %.4f'\n",
    "                      % (epoch, epochs, i, len(train_loader),\n",
    "                         errD, errG, D_x, D_G_z1, D_G_z2))\n",
    "\n",
    "        average_lossD = (sum_lossD / len(train_loader.dataset))\n",
    "        average_lossG = (sum_lossG / len(train_loader.dataset))\n",
    "        average_D_x = (sum_D_x / len(train_loader.dataset))\n",
    "        average_D_G_z = (sum_D_G_z / len(train_loader.dataset))\n",
    "\n",
    "        lossD_list.append(average_lossD)\n",
    "        lossG_list.append(average_lossG)            \n",
    "        D_x_list.append(average_D_x)\n",
    "        D_G_z_list.append(average_D_G_z)\n",
    "\n",
    "        print('==> Epoch: {} Average lossD: {:.10f} average_lossG: {:.10f},average D(x): {:.10f},average D(G(z)): {:.10f} '.format(epoch, average_lossD,average_lossG,average_D_x, average_D_G_z)) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() takes 3 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-279-3536bf8aa7bb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mprev_x\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mt1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprev_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: forward() takes 3 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "#############################\n",
    "# scratch pad\n",
    "#############################\n",
    "\n",
    "gen = Generator()\n",
    "gen.to(device)\n",
    "batch_size = 1\n",
    "z = torch.randn(batch_size, 100, device=device)\n",
    "\n",
    "prev_x = torch.unsqueeze(train[0][0], dim=0)\n",
    "prev_x.shape\n",
    "\n",
    "t1 = gen.forward(z, prev_x, batch_size)\n",
    "print(t1.shape)\n",
    "\n",
    "dis = Discriminator()\n",
    "dis.to(device)\n",
    "\n",
    "dis.forward(t1, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
